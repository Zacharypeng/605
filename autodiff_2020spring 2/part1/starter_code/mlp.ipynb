{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"22REMaeHT6kg","colab_type":"code","outputId":"c5df5587-9abd-43bd-adbd-f243eb000242","executionInfo":{"status":"ok","timestamp":1583791699921,"user_tz":240,"elapsed":537,"user":{"displayName":"Kuo Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64","userId":"12497217739166291065"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tEL9LC8wT76x","colab_type":"code","outputId":"e54e130c-b03a-448e-89ac-7c1bf93905d6","executionInfo":{"status":"ok","timestamp":1583791699922,"user_tz":240,"elapsed":527,"user":{"displayName":"Kuo Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64","userId":"12497217739166291065"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"drive/My Drive\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UZLkircqT7-T","colab_type":"code","outputId":"00725692-89b8-4b94-f35a-7af557d851f2","executionInfo":{"status":"ok","timestamp":1583791699923,"user_tz":240,"elapsed":515,"user":{"displayName":"Kuo Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64","userId":"12497217739166291065"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"part1\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/part1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CzMDbaIvUkTX","colab_type":"code","outputId":"4e24b9cf-2d9c-4b6e-8506-162924e14a64","executionInfo":{"status":"ok","timestamp":1583791699924,"user_tz":240,"elapsed":505,"user":{"displayName":"Kuo Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64","userId":"12497217739166291065"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"starter_code\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/part1/starter_code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CuS2dH6WT2rT","colab_type":"code","outputId":"bc4f5efa-4ba3-48c8-a640-f6761fe6ec02","executionInfo":{"status":"error","timestamp":1583791700840,"user_tz":240,"elapsed":1412,"user":{"displayName":"Kuo Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64","userId":"12497217739166291065"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["\"\"\"\n","Multilayer Perceptron for character level entity classification\n","\"\"\"\n","import argparse\n","import numpy as np\n","from xman import *\n","from utils import *\n","from autograd import *\n","\n","np.random.seed(0)\n","\n","EPS=1e-4\n","\n","def fwd(network, valueDict):\n","    ad = Autograd(network.my_xman)\n","    return ad.eval(network.my_xman.operationSequence(network.my_xman.loss), valueDict)\n","\n","def bwd(network, valueDict):\n","    ad = Autograd(network.my_xman)\n","    return ad.bprop(network.my_xman.operationSequence(network.my_xman.loss), valueDict,loss=np.float_(1.0))\n","\n","def update(network, dataParamDict, grads, rate):\n","    for rname in grads:\n","        if network.my_xman.isParam(rname):\n","            dataParamDict[rname] = dataParamDict[rname] - rate*grads[rname]\n","    return dataParamDict\n","\n","def accuracy(probs, targets):\n","    preds = np.argmax(probs, axis=1)\n","    targ = np.argmax(targets, axis=1)\n","    return float((preds==targ).sum())/preds.shape[0]\n","\n","def grad_check(network):\n","    # function which takes a network object and checks gradients\n","    # based on default values of data and params\n","    dataParamDict = network.my_xman.inputDict()\n","    fd = fwd(network, dataParamDict)\n","    grads = bwd(network, fd)\n","    for rname in grads:\n","        if network.my_xman.isParam(rname):\n","            fd[rname].ravel()[0] += EPS\n","            fp = fwd(network, fd)\n","            a = fp['loss']\n","            fd[rname].ravel()[0] -= 2*EPS\n","            fm = fwd(network, fd)\n","            b = fm['loss']\n","            fd[rname].ravel()[0] += EPS\n","            auto = grads[rname].ravel()[0]\n","            num = (a-b)/(2*EPS)\n","            if not np.isclose(auto, num, atol=1e-3):\n","                raise ValueError(\"gradients not close for %s, Auto %.5f Num %.5f\"\n","                        % (rname, auto, num))\n","\n","def glorot(m,n):\n","    # return scale for glorot initialization\n","    return np.sqrt(6./(m+n))\n","\n","class MLP(object):\n","    \"\"\"\n","    Multilayer Perceptron\n","    Accepts list of layer sizes [in_size, hid_size1, hid_size2, ..., out_size]\n","    \"\"\"\n","    def __init__(self, layer_sizes):\n","        self.num_layers = len(layer_sizes)-1\n","        self.my_xman = self._build(layer_sizes) # DO NOT REMOVE THIS LINE. Store the output of xman.setup() in this variable\n","        print (self.my_xman.operationSequence(self.my_xman.loss))\n","\n","    def _build(self, layer_sizes):\n","        print(\"INITIAZLIZING with layer_sizes:\", layer_sizes)\n","        self.params = {}\n","        for i in range(self.num_layers):\n","            k = i+1\n","            sc = glorot(layer_sizes[i], layer_sizes[i+1])\n","            self.params['W'+str(k)] = f.param(name='W'+str(k),\n","                    default=sc*np.random.uniform(low=-1.,high=1.,\n","                        size=(layer_sizes[i], layer_sizes[i+1])))\n","            self.params['b'+str(k)] = f.param(name='b'+str(k),\n","                    default=0.1*np.random.uniform(low=-1.,high=1.,size=(layer_sizes[i+1],)))\n","        self.inputs = {}\n","        self.inputs['X'] = f.input(name='X', default=np.random.rand(1,layer_sizes[0]))\n","        self.inputs['y'] = f.input(name='y', default=np.random.rand(1,layer_sizes[-1]))\n","        x = XMan()\n","        inp = self.inputs['X']\n","        for i in range(self.num_layers):\n","            oo = f.mul(inp,self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n","            inp = f.relu( oo )\n","\n","        x.output = f.softMax(inp)\n","        # loss\n","        x.loss = f.mean(f.crossEnt(x.output, self.inputs['y']))\n","        return x.setup()\n","\n","    def data_dict(self, X, y):\n","        dataDict = {}\n","        dataDict['X'] = X\n","        dataDict['y'] = y\n","        return dataDict\n","\n","def main(params):\n","    epochs = params['epochs']\n","    max_len = params['max_len']\n","    num_hid = params['num_hid']\n","    batch_size = params['batch_size']\n","    dataset = params['dataset']\n","    init_lr = params['init_lr']\n","    output_file = params['output_file']\n","    train_loss_file = params['train_loss_file']\n","\n","    # load data and preprocess\n","    dp = DataPreprocessor()\n","    data = dp.preprocess('%s.train'%dataset, '%s.valid'%dataset, '%s.test'%dataset)\n","    # minibatches\n","    mb_train = MinibatchLoader(data.training, batch_size, max_len,\n","           len(data.chardict), len(data.labeldict))\n","    mb_valid = MinibatchLoader(data.validation, len(data.validation), max_len,\n","           len(data.chardict), len(data.labeldict), shuffle=False)\n","    mb_test = MinibatchLoader(data.test, len(data.test), max_len,\n","           len(data.chardict), len(data.labeldict), shuffle=False)\n","\n","    # build\n","    print (\"building mlp...\")\n","    mlp = MLP([max_len*mb_train.num_chars,num_hid,mb_train.num_labels])\n","    grad_check(mlp)\n","\n","    print (\"done\")\n","\n","    # train\n","    print (\"training...\")\n","    logger = open('%s_mlp4c_L%d_H%d_B%d_E%d_lr%.3f.txt'%\n","            (dataset,max_len,num_hid,batch_size,epochs,init_lr),'w')\n","    # get default data and params\n","    value_dict = mlp.my_xman.inputDict()\n","    min_loss = 1e5\n","    lr = init_lr\n","    train_loss = np.ndarray([0])\n","    best_param_dict = {}\n","    for i in range(epochs):\n","        for ii, (idxs,e,l) in enumerate(mb_train):\n","            # prepare input\n","            data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n","            for k,v in data_dict.iteritems():\n","                value_dict[k] = v\n","            # fwd-bwd\n","            vd = fwd(mlp,value_dict)\n","            gd = bwd(mlp,value_dict)\n","            value_dict = update(mlp, value_dict, gd, lr)\n","            message = 'TRAIN loss = %.3f' % vd['loss']\n","            logger.write(message+'\\n')\n","            train_loss = np.append(train_loss, vd['loss'])\n","        print (ii)\n","        # validate\n","        tot_loss, n= 0., 0\n","        probs = []\n","        targets = []\n","        for (idxs,e,l) in mb_valid:\n","            # prepare input\n","            data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n","            for k,v in data_dict.iteritems():\n","                value_dict[k] = v\n","            # fwd\n","            vd = fwd(mlp, value_dict)\n","            tot_loss += vd['loss']\n","            probs.append(vd['output'])\n","            targets.append(l)\n","            n += 1\n","        acc = accuracy(np.vstack(probs), np.vstack(targets))\n","        c_loss = tot_loss/n\n","        if c_loss<min_loss:\n","            min_loss = c_loss\n","            for k,v in value_dict.iteritems():\n","                best_param_dict[k] = np.copy(v)\n","        message = ('Epoch %d VAL loss %.3f min_loss %.3f acc %.3f' %\n","                (i,c_loss,min_loss,acc))\n","        logger.write(message+'\\n')\n","        print (message)\n","\n","    np.save(train_loss_file, train_loss)\n","\n","    tot_loss, n = 0., 0\n","    probs = []\n","    targets = []\n","    for (idxs,e,l) in mb_test:\n","        # prepare input\n","        data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n","        for k,v in data_dict.iteritems():\n","            best_param_dict[k] = v\n","        # fwd\n","        vd = fwd(mlp,best_param_dict)\n","        tot_loss += vd['loss']\n","        probs.append(vd['output'])\n","        targets.append(l)\n","        n += 1\n","    acc = accuracy(np.vstack(probs), np.vstack(targets))\n","    c_loss = tot_loss/n\n","    np.save(output_file, np.vstack(probs))\n","    print (\"done, test loss = %.3f acc = %.3f\" % (c_loss, acc))\n","\n","\n","# parser = argparse.ArgumentParser()\n","# parser.add_argument('--max_len', dest='max_len', type=int, default=10)\n","# parser.add_argument('--num_hid', dest='num_hid', type=int, default=50)\n","# parser.add_argument('--batch_size', dest='batch_size', type=int, default=64)\n","# parser.add_argument('--dataset', dest='dataset', type=str, default='tiny')\n","# parser.add_argument('--epochs', dest='epochs', type=int, default=15)\n","# parser.add_argument('--init_lr', dest='init_lr', type=float, default=0.5)\n","# parser.add_argument('--output_file', dest='output_file', type=str, default='output')\n","# parser.add_argument('--train_loss_file', dest='train_loss_file', type=str, default='train_loss')\n","# params = vars(parser.parse_args())\n","params = dict()\n","params['max_len'] = 10\n","params['num_hid'] = 50\n","params['batch_size'] = 64\n","params['dataset'] = 'tiny'\n","params['epochs'] = 15\n","params['init_lr'] = 0.5\n","params['output_file'] = 'output'\n","params['train_loss_file'] = 'train_loss'\n","main(params)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--max_len MAX_LEN] [--num_hid NUM_HID]\n","                             [--batch_size BATCH_SIZE] [--dataset DATASET]\n","                             [--epochs EPOCHS] [--init_lr INIT_LR]\n","                             [--output_file OUTPUT_FILE]\n","                             [--train_loss_file TRAIN_LOSS_FILE]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-742627cc-201e-4541-bc61-c2aad31efea3.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fSYBRtL7T8hb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}