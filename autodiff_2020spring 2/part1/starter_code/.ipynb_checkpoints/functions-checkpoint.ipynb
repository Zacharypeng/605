{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1583794162153,
     "user": {
      "displayName": "Kuo Tian",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64",
      "userId": "12497217739166291065"
     },
     "user_tz": 240
    },
    "id": "pAcLLqawMTgg",
    "outputId": "11128689-192b-4677-bbc5-f3c11e16f95c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1583794176171,
     "user": {
      "displayName": "Kuo Tian",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi7kLEyoiYcViwV9lnFvnPz_VhVQ0lSfFQrqZhD=s64",
      "userId": "12497217739166291065"
     },
     "user_tz": 240
    },
    "id": "ck5fosmBNKYf",
    "outputId": "1293da10-8fc6-44de-b4bf-91b509f1f598"
   },
   "outputs": [],
   "source": [
    "# %cd \"drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 364,
     "status": "error",
     "timestamp": 1583796169519,
     "user": {
      "displayName": "Zeyu Peng",
      "photoUrl": "",
      "userId": "08059898068345402060"
     },
     "user_tz": 240
    },
    "id": "s8NSGdqblv0B",
    "outputId": "159d0d18-2272-4b79-a21d-35d0af70b730"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1583793779441,
     "user": {
      "displayName": "Zeyu Peng",
      "photoUrl": "",
      "userId": "08059898068345402060"
     },
     "user_tz": 240
    },
    "id": "xuQsQHJINrFh",
    "outputId": "073d7245-cf0d-47af-c2ed-5daf3dc85f60"
   },
   "outputs": [],
   "source": [
    "# %cd \"part1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1583793780038,
     "user": {
      "displayName": "Zeyu Peng",
      "photoUrl": "",
      "userId": "08059898068345402060"
     },
     "user_tz": 240
    },
    "id": "3t_Bz8U3PJ_f",
    "outputId": "34b068b4-358e-45c9-a8d3-f988ebc048e2"
   },
   "outputs": [],
   "source": [
    "# %cd \"starter_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AX8JwFLQOCV"
   },
   "outputs": [],
   "source": [
    "# some useful functions\n",
    "import numpy as np\n",
    "from xman import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Svld7i6iRVzO"
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL IN> with appropriate code to declare all operations\n",
    "\n",
    "class f(XManFunctions):\n",
    "    @staticmethod\n",
    "    def square(a):\n",
    "        return XManFunctions.registerDefinedByOperator('square',a)\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def mean(a):\n",
    "    #     return <FILL_IN>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    @staticmethod\n",
    "    def mean(a):\n",
    "        return XManFunctions.registerDefinedByOperator('mean',a)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def softMax(a):\n",
    "    #     return <FILL_IN>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    @staticmethod\n",
    "    def softMax(a):\n",
    "        return XManFunctions.registerDefinedByOperator('softMax',a)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # @staticmethod\n",
    "    # def crossEnt(a):\n",
    "    #     return <FILL_IN>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    @staticmethod\n",
    "    def crossEnt(a,b):\n",
    "        return XManFunctions.registerDefinedByOperator('crossEnt',a,b)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # @staticmethod\n",
    "    # def relu(a):\n",
    "    #     return <FILL_IN>\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    @staticmethod\n",
    "    def relu(a):\n",
    "        return XManFunctions.registerDefinedByOperator('relu',a)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoSJN9zDRXlE"
   },
   "outputs": [],
   "source": [
    "# the functions that autograd.eval will use to evaluate each function,\n",
    "# to be called with the functions actual inputs as arguments\n",
    "\n",
    "def _softMax(x):\n",
    "    maxes = np.amax(x, axis=1)\n",
    "    # print (\"line number 35\", x.shape, maxes.shape)\n",
    "    maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "    # print (\"line number 37\", maxes.shape)\n",
    "    e_x = np.exp(x - maxes)\n",
    "    sums = np.sum(e_x, axis=1)\n",
    "    # print (\"line number 40\",  e_x.shape, sums.shape)\n",
    "    sums = sums.reshape(sums.shape[0], 1)\n",
    "    # print (\"line number 42\", sums.shape)\n",
    "    dist = e_x / sums\n",
    "    return dist\n",
    "\n",
    "def _crossEnt(x,y):\n",
    "    log_x = np.nan_to_num(np.log(x))\n",
    "    return - np.multiply(y,log_x).sum(axis=1, keepdims=True)\n",
    "\n",
    "EVAL_FUNS = {\n",
    "    'add':      lambda x1,x2: x1+x2,\n",
    "    'subtract': lambda x1,x2: x1-x2,\n",
    "    'square':   np.square,\n",
    "    'mul':      lambda x1,x2: np.dot(x1,x2),\n",
    "    'mean':     lambda x:x.mean(),\n",
    "    'softMax':  _softMax,\n",
    "    'crossEnt': _crossEnt,\n",
    "    'relu': lambda x: np.maximum(0,x)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsoIyL5xRdGF"
   },
   "outputs": [],
   "source": [
    "# the functions that autograd.bprop will use in reverse mode\n",
    "# differentiation.  BP_FUNS[f] is a list of functions df1,....,dfk\n",
    "# where dfi is used in propagating errors to the i-th input xi of f.\n",
    "# Specifically, dfi is called with the ordinary inputs to f, with two\n",
    "# additions: the incoming error, and the output of the function, which\n",
    "# was computed by autograd.eval in the eval stage.  dfi will return\n",
    "# delta * df/dxi [f(x1,...,xk)]\n",
    "#\n",
    "# NOTE: Autograd has an optimization where if it finds a softMax op\n",
    "# followed by crossEnt op, it combines the backward pass for both. So\n",
    "# you only need to implement the BP_FUNS for the combined operation\n",
    "# crossEnt-softMax below.\n",
    "\n",
    "def _derivDot1(delta,out,x1,x2):\n",
    "    return np.dot(delta, x2.transpose())\n",
    "\n",
    "def _derivDot2(delta,out,x1,x2):\n",
    "    return np.dot(x1.transpose(), delta)\n",
    "\n",
    "def _derivAdd(delta,x1):\n",
    "    if delta.shape!=x1.shape:\n",
    "        # broadcast, sum along axis=0\n",
    "        if delta.shape[1]!=x1.shape[0]:\n",
    "            raise ValueError(\"Dimension Mismatch\")\n",
    "        return delta.sum(axis=0) #we sum the gradients over the batch\n",
    "    else: return delta\n",
    "\n",
    "def _derivSoftMax(delta,out,x):\n",
    "    return (delta[:,:,None]*(out[:,:,None]*(np.eye(out.shape[1])[None,:,:] - out[:,None,:]))).sum(axis=1)\n",
    "\n",
    "def _derivCrossEnt1(delta,out,x,y):\n",
    "    return -y*np.reciprocal(x)\n",
    "\n",
    "def _derivCrossEnt2(delta,out,x,y):\n",
    "    return -np.log(x)\n",
    "\n",
    "BP_FUNS = {\n",
    "    'add':              [lambda delta,out,x1,x2: _derivAdd(delta,x1),    lambda delta,out,x1,x2: _derivAdd(delta,x2)],\n",
    "    'subtract':         [lambda delta,out,x1,x2: _derivAdd(delta,x1),    lambda delta,out,x1,x2: -_derivAdd(delta,x2)],\n",
    "    'square':           [lambda delta,out,x : delta * 2.0 * x],\n",
    "    'mul':              [_derivDot1, _derivDot2],\n",
    "    'mean':             [lambda delta,out,x : delta * 1.0/float(x.shape[0])*np.ones(x.shape)],\n",
    "    'relu':             [lambda delta,out,x : delta * ((x>0).astype(np.float64))],\n",
    "    'softMax':          [_derivSoftMax],\n",
    "    'crossEnt':         [_derivCrossEnt1, _derivCrossEnt2],\n",
    "    'crossEnt-softMax': [lambda delta,out,x,y: delta*(_softMax(x)*y.sum(axis=1)[:,None] - y),  lambda delta,out,x,y:-delta*np.log(_softMax(x))],  #second one is never used for much\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Zd6UWpyPXAy"
   },
   "outputs": [],
   "source": [
    "# Unit tests for the functions. Run by `python functions.py`.\n",
    "x = np.array([\n",
    "    [ 0.76677119,  0.12815245],\n",
    "    [ 0.4007303 ,  0.77046941],\n",
    "    [ 0.00574018,  0.71242641]])\n",
    "y = np.array([\n",
    "    [-0.06655641,  0.10877971],\n",
    "    [ 0.13663944, -0.12461873]])\n",
    "z = np.array([[0., 1.], [0., 1.], [1., 0.]])\n",
    "v =np.array([[ 0.96894013], [ 0.07382228]])\n",
    "# Eval mul\n",
    "expected_x_mul_y =  np.array([[-0.03352286,  0.06743895],\n",
    "    [ 0.07860534, -0.05242359],\n",
    "    [ 0.0969635 , -0.08815726]])\n",
    "np.testing.assert_allclose(EVAL_FUNS['mul'](x, y), expected_x_mul_y)\n",
    "expected_relu_y = np.array([\n",
    "    [ 0.        ,  0.10877971],\n",
    "    [ 0.13663944,  0.        ]])\n",
    "# Eval relu\n",
    "np.testing.assert_allclose(EVAL_FUNS['relu'](y), expected_relu_y)\n",
    "expected_softMax_x = np.array([\n",
    "    [ 0.65444116,  0.34555884],\n",
    "    [ 0.40860406,  0.59139594],\n",
    "    [ 0.33033148,  0.66966852]])\n",
    "# Eval softMax\n",
    "np.testing.assert_allclose(EVAL_FUNS['softMax'](x), expected_softMax_x)\n",
    "expected_crossEnt_softMax_x_z = np.array([\n",
    "    [ 1.06259235],\n",
    "    [ 0.52526954],\n",
    "    [ 1.10765864]])\n",
    "# Eval crossEnt\n",
    "np.testing.assert_allclose(EVAL_FUNS['crossEnt'](expected_softMax_x, z), expected_crossEnt_softMax_x_z)\n",
    "# Eval mean\n",
    "expected_mean_v = 0.52138120499999996\n",
    "np.testing.assert_allclose(EVAL_FUNS['mean'](v), expected_mean_v)\n",
    "# BP mul\n",
    "delta_x_mul_y = np.array([\n",
    "    [ 0.12523631,  0.00680066],\n",
    "    [ 0.48109275,  0.95663136],\n",
    "    [ 0.40436419,  0.56481742]])\n",
    "np.testing.assert_allclose(BP_FUNS['mul'][0](delta_x_mul_y, expected_x_mul_y, x, y), np.array([\n",
    "    [-0.00759551,  0.01626473],\n",
    "    [ 0.07204228, -0.05347794],\n",
    "    [ 0.03452765, -0.01513473]]), rtol=1e-06)\n",
    "np.testing.assert_allclose(BP_FUNS['mul'][1](delta_x_mul_y, expected_x_mul_y, x, y), np.array([\n",
    "    [ 0.29113716,  0.39180788],\n",
    "    [ 0.67479632,  1.14031757]]))\n",
    "# BP relu\n",
    "delta_relu_y = np.array([\n",
    "    [ 0.66202207,  0.59765468],\n",
    "    [ 0.01812402,  0.58537534]])\n",
    "np.testing.assert_allclose(BP_FUNS['relu'][0](delta_relu_y, expected_relu_y, y), np.array([\n",
    "    [ 0.        ,  0.59765468],\n",
    "    [ 0.01812402,  0.        ]]))\n",
    "# BP crossEnt-softMax\n",
    "delta_crossEnt_softMax_x_z = np.array([\n",
    "    [  5.69906247e-01],\n",
    "    [  8.66851385e-01],\n",
    "    [  2.79581480e-04]])\n",
    "np.testing.assert_allclose(BP_FUNS['crossEnt-softMax'][0](delta_crossEnt_softMax_x_z, expected_crossEnt_softMax_x_z, x, z), np.array([\n",
    "    [  3.72970104e-01,  -3.72970104e-01],\n",
    "    [  3.54198998e-01,  -3.54198998e-01],\n",
    "    [ -1.87226917e-04,   1.87226917e-04]]))\n",
    "# BP mean\n",
    "np.testing.assert_allclose(BP_FUNS['mean'][0](0.19950823, expected_mean_v, v), np.array([\n",
    "    [ 0.09975412],\n",
    "    [ 0.09975412]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1kmG3-uRq_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing vocabulary\n",
      "# chars in training  132\n",
      "# chars in validation  78\n",
      "# chars in testing  78\n",
      "# chars in (testing-training-validation)  0\n",
      "# labels 5\n",
      "preparing training data\n",
      "num_rows: 1857  index 1857\n",
      "preparing validation data\n",
      "num_rows: 221  index 221\n",
      "preparing test data\n",
      "num_rows: 221  index 221\n",
      "building mlp...\n",
      "INITIAZLIZING with layer_sizes: [1340, 50, 5]\n",
      "[('z7', 'mul', <map object at 0x11bd58490>), ('z6', 'add', <map object at 0x11bd58510>), ('z5', 'relu', <map object at 0x11bd585d0>), ('z4', 'mul', <map object at 0x11bd58650>), ('z3', 'add', <map object at 0x11bd58710>), ('z2', 'relu', <map object at 0x11bd58790>), ('output', 'softMax', <map object at 0x11bd587d0>), ('z1', 'crossEnt', <map object at 0x11bd58850>), ('loss', 'mean', <map object at 0x11bd588d0>)]\n",
      "first\n",
      "> /Users/zacharypeng/Downloads/part1/starter_code/autograd.py(86)optimizeForBProp()\n",
      "-> mapList = [i for i in opseq[k][2]]\n",
      "(Pdb) k\n",
      "1\n",
      "(Pdb) opseq[k][0]\n",
      "'z1'\n",
      "(Pdb) opseq[k][1]\n",
      "'crossEnt'\n",
      "(Pdb) opseq[2][0]\n",
      "'output'\n",
      "(Pdb) opseq[2][2]\n",
      "<map object at 0x11bd58510>\n",
      "(Pdb) opseq[2][2].keys()\n",
      "*** AttributeError: 'map' object has no attribute 'keys'\n",
      "(Pdb) opseq[2][2].key()\n",
      "*** AttributeError: 'map' object has no attribute 'key'\n",
      "(Pdb) print([a for a in  opseq[2][2]])\n",
      "['z2']\n",
      "(Pdb) opseq[2][2]['z2']\n",
      "*** TypeError: 'map' object is not subscriptable\n",
      "(Pdb) typeof(opseq[2][2])\n",
      "*** NameError: name 'typeof' is not defined\n",
      "(Pdb) python version\n",
      "*** SyntaxError: invalid syntax\n",
      "(Pdb) python --version\n",
      "*** NameError: name 'python' is not defined\n",
      "(Pdb) python -version\n",
      "*** NameError: name 'python' is not defined\n",
      "(Pdb) len(opseq[k][2])\n",
      "*** TypeError: object of type 'map' has no len()\n",
      "(Pdb) opseq[k][2].size()\n",
      "*** AttributeError: 'map' object has no attribute 'size'\n",
      "(Pdb) opseq[k][2].size\n",
      "*** AttributeError: 'map' object has no attribute 'size'\n",
      "(Pdb) k\n",
      "1\n",
      "(Pdb) print([a for a in  opseq[k][2]])\n",
      "[]\n",
      "(Pdb) opseq\n",
      "[('loss', 'mean', <map object at 0x11bd58410>), ('z1', 'crossEnt', <map object at 0x11bd58910>), ('output', 'softMax', <map object at 0x11bd58510>), ('z2', 'relu', <map object at 0x11bd58390>), ('z3', 'add', <map object at 0x11bd58890>), ('z4', 'mul', <map object at 0x11bd588d0>), ('z5', 'relu', <map object at 0x11bd585d0>), ('z6', 'add', <map object at 0x11bd58850>), ('z7', 'mul', <map object at 0x11bd58590>)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multilayer Perceptron for character level entity classification\n",
    "\"\"\"\n",
    "import argparse\n",
    "import numpy as np\n",
    "from xman import *\n",
    "from utils import *\n",
    "from autograd import *\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "EPS=1e-4\n",
    "\n",
    "def fwd(network, valueDict):\n",
    "    ad = Autograd(network.my_xman)\n",
    "    return ad.eval(network.my_xman.operationSequence(network.my_xman.loss), valueDict)\n",
    "\n",
    "def bwd(network, valueDict):\n",
    "    ad = Autograd(network.my_xman)\n",
    "    return ad.bprop(network.my_xman.operationSequence(network.my_xman.loss), valueDict,loss=np.float_(1.0))\n",
    "\n",
    "def update(network, dataParamDict, grads, rate):\n",
    "    for rname in grads:\n",
    "        if network.my_xman.isParam(rname):\n",
    "            dataParamDict[rname] = dataParamDict[rname] - rate*grads[rname]\n",
    "    return dataParamDict\n",
    "\n",
    "def accuracy(probs, targets):\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    targ = np.argmax(targets, axis=1)\n",
    "    return float((preds==targ).sum())/preds.shape[0]\n",
    "\n",
    "def grad_check(network):\n",
    "    # function which takes a network object and checks gradients\n",
    "    # based on default values of data and params\n",
    "    dataParamDict = network.my_xman.inputDict()\n",
    "    fd = fwd(network, dataParamDict)\n",
    "    grads = bwd(network, fd)\n",
    "    for rname in grads:\n",
    "        if network.my_xman.isParam(rname):\n",
    "            fd[rname].ravel()[0] += EPS\n",
    "            fp = fwd(network, fd)\n",
    "            a = fp['loss']\n",
    "            fd[rname].ravel()[0] -= 2*EPS\n",
    "            fm = fwd(network, fd)\n",
    "            b = fm['loss']\n",
    "            fd[rname].ravel()[0] += EPS\n",
    "            auto = grads[rname].ravel()[0]\n",
    "            num = (a-b)/(2*EPS)\n",
    "            if not np.isclose(auto, num, atol=1e-3):\n",
    "                raise ValueError(\"gradients not close for %s, Auto %.5f Num %.5f\"\n",
    "                        % (rname, auto, num))\n",
    "\n",
    "def glorot(m,n):\n",
    "    # return scale for glorot initialization\n",
    "    return np.sqrt(6./(m+n))\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron\n",
    "    Accepts list of layer sizes [in_size, hid_size1, hid_size2, ..., out_size]\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.num_layers = len(layer_sizes)-1\n",
    "        self.my_xman = self._build(layer_sizes) # DO NOT REMOVE THIS LINE. Store the output of xman.setup() in this variable\n",
    "        print (self.my_xman.operationSequence(self.my_xman.loss))\n",
    "\n",
    "    def _build(self, layer_sizes):\n",
    "        print(\"INITIAZLIZING with layer_sizes:\", layer_sizes)\n",
    "        self.params = {}\n",
    "        for i in range(self.num_layers):\n",
    "            k = i+1\n",
    "            sc = glorot(layer_sizes[i], layer_sizes[i+1])\n",
    "            self.params['W'+str(k)] = f.param(name='W'+str(k),\n",
    "                    default=sc*np.random.uniform(low=-1.,high=1.,\n",
    "                        size=(layer_sizes[i], layer_sizes[i+1])))\n",
    "            self.params['b'+str(k)] = f.param(name='b'+str(k),\n",
    "                    default=0.1*np.random.uniform(low=-1.,high=1.,size=(layer_sizes[i+1],)))\n",
    "        self.inputs = {}\n",
    "        self.inputs['X'] = f.input(name='X', default=np.random.rand(1,layer_sizes[0]))\n",
    "        self.inputs['y'] = f.input(name='y', default=np.random.rand(1,layer_sizes[-1]))\n",
    "        x = XMan()\n",
    "        inp = self.inputs['X']\n",
    "        for i in range(self.num_layers):\n",
    "            oo = f.mul(inp,self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n",
    "            inp = f.relu( oo )\n",
    "\n",
    "        x.output = f.softMax(inp)\n",
    "        # loss\n",
    "        x.loss = f.mean(f.crossEnt(x.output, self.inputs['y']))\n",
    "        return x.setup()\n",
    "\n",
    "    def data_dict(self, X, y):\n",
    "        dataDict = {}\n",
    "        dataDict['X'] = X\n",
    "        dataDict['y'] = y\n",
    "        return dataDict\n",
    "\n",
    "def main(params):\n",
    "    epochs = params['epochs']\n",
    "    max_len = params['max_len']\n",
    "    num_hid = params['num_hid']\n",
    "    batch_size = params['batch_size']\n",
    "    dataset = params['dataset']\n",
    "    init_lr = params['init_lr']\n",
    "    output_file = params['output_file']\n",
    "    train_loss_file = params['train_loss_file']\n",
    "\n",
    "    # load data and preprocess\n",
    "    dp = DataPreprocessor()\n",
    "    data = dp.preprocess('%s.train'%dataset, '%s.valid'%dataset, '%s.test'%dataset)\n",
    "    # minibatches\n",
    "    mb_train = MinibatchLoader(data.training, batch_size, max_len,\n",
    "           len(data.chardict), len(data.labeldict))\n",
    "    mb_valid = MinibatchLoader(data.validation, len(data.validation), max_len,\n",
    "           len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "    mb_test = MinibatchLoader(data.test, len(data.test), max_len,\n",
    "           len(data.chardict), len(data.labeldict), shuffle=False)\n",
    "\n",
    "    # build\n",
    "    print (\"building mlp...\")\n",
    "    mlp = MLP([max_len*mb_train.num_chars,num_hid,mb_train.num_labels])\n",
    "    grad_check(mlp)\n",
    "\n",
    "    print (\"done\")\n",
    "\n",
    "    # train\n",
    "    print (\"training...\")\n",
    "    logger = open('%s_mlp4c_L%d_H%d_B%d_E%d_lr%.3f.txt'%\n",
    "            (dataset,max_len,num_hid,batch_size,epochs,init_lr),'w')\n",
    "    # get default data and params\n",
    "    value_dict = mlp.my_xman.inputDict()\n",
    "    min_loss = 1e5\n",
    "    lr = init_lr\n",
    "    train_loss = np.ndarray([0])\n",
    "    best_param_dict = {}\n",
    "    for i in range(epochs):\n",
    "        for ii, (idxs,e,l) in enumerate(mb_train):\n",
    "            # prepare input\n",
    "            data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n",
    "            for k,v in data_dict.iteritems():\n",
    "                value_dict[k] = v\n",
    "            # fwd-bwd\n",
    "            vd = fwd(mlp,value_dict)\n",
    "            gd = bwd(mlp,value_dict)\n",
    "            value_dict = update(mlp, value_dict, gd, lr)\n",
    "            message = 'TRAIN loss = %.3f' % vd['loss']\n",
    "            logger.write(message+'\\n')\n",
    "            train_loss = np.append(train_loss, vd['loss'])\n",
    "        print (ii)\n",
    "        # validate\n",
    "        tot_loss, n= 0., 0\n",
    "        probs = []\n",
    "        targets = []\n",
    "        for (idxs,e,l) in mb_valid:\n",
    "            # prepare input\n",
    "            data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n",
    "            for k,v in data_dict.iteritems():\n",
    "                value_dict[k] = v\n",
    "            # fwd\n",
    "            vd = fwd(mlp, value_dict)\n",
    "            tot_loss += vd['loss']\n",
    "            probs.append(vd['output'])\n",
    "            targets.append(l)\n",
    "            n += 1\n",
    "        acc = accuracy(np.vstack(probs), np.vstack(targets))\n",
    "        c_loss = tot_loss/n\n",
    "        if c_loss<min_loss:\n",
    "            min_loss = c_loss\n",
    "            for k,v in value_dict.iteritems():\n",
    "                best_param_dict[k] = np.copy(v)\n",
    "        message = ('Epoch %d VAL loss %.3f min_loss %.3f acc %.3f' %\n",
    "                (i,c_loss,min_loss,acc))\n",
    "        logger.write(message+'\\n')\n",
    "        print (message)\n",
    "\n",
    "    np.save(train_loss_file, train_loss)\n",
    "\n",
    "    tot_loss, n = 0., 0\n",
    "    probs = []\n",
    "    targets = []\n",
    "    for (idxs,e,l) in mb_test:\n",
    "        # prepare input\n",
    "        data_dict = mlp.data_dict(e.reshape((e.shape[0],e.shape[1]*e.shape[2])),l)\n",
    "        for k,v in data_dict.iteritems():\n",
    "            best_param_dict[k] = v\n",
    "        # fwd\n",
    "        vd = fwd(mlp,best_param_dict)\n",
    "        tot_loss += vd['loss']\n",
    "        probs.append(vd['output'])\n",
    "        targets.append(l)\n",
    "        n += 1\n",
    "    acc = accuracy(np.vstack(probs), np.vstack(targets))\n",
    "    c_loss = tot_loss/n\n",
    "    np.save(output_file, np.vstack(probs))\n",
    "    print (\"done, test loss = %.3f acc = %.3f\" % (c_loss, acc))\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--max_len', dest='max_len', type=int, default=10)\n",
    "# parser.add_argument('--num_hid', dest='num_hid', type=int, default=50)\n",
    "# parser.add_argument('--batch_size', dest='batch_size', type=int, default=64)\n",
    "# parser.add_argument('--dataset', dest='dataset', type=str, default='tiny')\n",
    "# parser.add_argument('--epochs', dest='epochs', type=int, default=15)\n",
    "# parser.add_argument('--init_lr', dest='init_lr', type=float, default=0.5)\n",
    "# parser.add_argument('--output_file', dest='output_file', type=str, default='output')\n",
    "# parser.add_argument('--train_loss_file', dest='train_loss_file', type=str, default='train_loss')\n",
    "# params = vars(parser.parse_args())\n",
    "params = dict()\n",
    "params['max_len'] = 10\n",
    "params['num_hid'] = 50\n",
    "params['batch_size'] = 64\n",
    "params['dataset'] = '../../data/tiny'\n",
    "params['epochs'] = 15\n",
    "params['init_lr'] = 0.5\n",
    "params['output_file'] = 'output'\n",
    "params['train_loss_file'] = 'train_loss'\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
